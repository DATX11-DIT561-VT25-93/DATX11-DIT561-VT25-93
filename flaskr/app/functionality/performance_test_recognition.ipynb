{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load feature vectors from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from detection import detect_face_for_testing\n",
    "from feature_extraction import extract_feature, init_facenet\n",
    "\n",
    "DATASET_PATH = \"test_images_LFW_entire\"\n",
    "REF_IMG_INDEX = 0 # The index for the reference images in a particular dataset\n",
    "                   # FEI -> 11\n",
    "                   # LFW -> 0\n",
    "                   # GT  -> 0\n",
    "\n",
    "recognition_model = init_facenet()\n",
    "\n",
    "ref_feat_vect_dict = {} # Dictionary to store the reference feature vector for each person\n",
    "feat_vect_dict = {} # Dictionary to store the rest of the feature vectors for each person\n",
    "\n",
    "#j = 0\n",
    "\n",
    "for person in os.listdir(DATASET_PATH):\n",
    "    person_path = os.path.join(DATASET_PATH, person)\n",
    "\n",
    "    feat_vect_dict[person] = []\n",
    "\n",
    "    #j += 1\n",
    "\n",
    "    for i, img_name in enumerate(os.listdir(person_path)):\n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        img_array = cv2.imread(img_path)\n",
    "        face_data, _, image_rgb = detect_face_for_testing(img_array)\n",
    "        \n",
    "        if face_data is not None:\n",
    "            feature_vector = extract_feature(face_data, image_rgb, recognition_model)\n",
    "\n",
    "            if feature_vector is None:\n",
    "                continue\n",
    "\n",
    "            if i == REF_IMG_INDEX:\n",
    "                ref_feat_vect_dict[person] = feature_vector # Add reference feature vector to its corresponding person\n",
    "            else:\n",
    "                feat_vect_dict[person].append(feature_vector) # Add feature vector to its corresponding person\n",
    "        else:\n",
    "            print(\"No face detected\")\n",
    "            \n",
    "    #if j > 10: # To ensure not loading the entire dataset when testing the basic test functionality\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "# Create positive test for each person cases by matching the reference image with each of the other images\n",
    "pos_test_cases = []\n",
    "\n",
    "for person, feature_vectors in feat_vect_dict.items():\n",
    "    ref_feat_vect = ref_feat_vect_dict.get(person)\n",
    "\n",
    "    if ref_feat_vect is None:\n",
    "        continue\n",
    "\n",
    "    for i in range(len(feature_vectors)):\n",
    "        feat_vect = feature_vectors[i]\n",
    "        pos_test_cases.append((ref_feat_vect, feat_vect, True, person, person)) # Add test case consisting of the feature vectors that are to be compared and the expected value\n",
    "\n",
    "# Create negative tests by randomly matching non-identical reference images. Should constitute 20% of the total test cases\n",
    "num_of_neg_tests = int(len(pos_test_cases)/4)\n",
    "\n",
    "neg_pairs = list(combinations(ref_feat_vect_dict.items(), 2))\n",
    "neg_pairs = neg_pairs[:num_of_neg_tests]\n",
    "neg_test_cases = [(ref_feat_vect1, ref_feat_vect2, False, person1, person2) for (person1, ref_feat_vect1), (person2, ref_feat_vect2) in neg_pairs]\n",
    "random.shuffle(neg_test_cases)\n",
    "\n",
    "# Combine the test cases\n",
    "all_test_cases = pos_test_cases + neg_test_cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Functions used to evaluate the test cases and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from verification import compare_faces_euclidean\n",
    "import numpy as np\n",
    "\n",
    "VERIFICATION_THRESHOLDS = np.arange(0.1, 2.1, 0.1) # Used for comparing the likeness of two feature vectors\n",
    "\n",
    "# Global font\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Global font size\n",
    "mpl.rcParams['font.size'] = 18\n",
    "\n",
    "def run_test(y_true, threshold):\n",
    "    y_pred = []\n",
    "\n",
    "    # Test the verification functionality on each test case pair \n",
    "    for f_vect_1, f_vect_2, has_same_face, p1, p2 in all_test_cases:\n",
    "        prediction = compare_faces_euclidean(f_vect_1, f_vect_2, threshold=threshold) #prediction, distance = compare_faces_euclidean(f_vect_1, f_vect_2)\n",
    "        y_pred.append(prediction)\n",
    "        if prediction == has_same_face:\n",
    "            pass\n",
    "        else:\n",
    "            #print(f'Incorrect prediction for persons {p1} and {p2}')\n",
    "            pass\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"For treshold: {threshold}\\nAccuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\\n\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def plot_conf_mat(y_true, threshold):\n",
    "    y_pred = run_test(y_true, threshold)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    conf_mat_display = ConfusionMatrixDisplay(conf_mat)\n",
    "    conf_mat_display.plot()\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.ylabel(\"True Value\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_tpr_and_fpr(y_true, y_pred):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "\n",
    "    tpr = tp / (tp + fn)  # True positive rate\n",
    "    fpr = fp / (fp + tn)  # False positive rate\n",
    "\n",
    "    return tpr, fpr\n",
    "\n",
    "def plot_ROC(y_true):\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "\n",
    "    for threshold in VERIFICATION_THRESHOLDS:\n",
    "        y_pred = run_test(y_true, threshold)\n",
    "        \n",
    "        tpr, fpr = get_tpr_and_fpr(y_true, y_pred)\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    plt.figure(figsize=(5.43, 4.65)) \n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    plt.plot(fpr_list, tpr_list, marker='o', linestyle='-', label='ROC Curve', alpha=0.7, markersize=8)\n",
    "\n",
    "    # Plotting the diagonal (random guess line)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # Diagonal line from (0, 0) to (1, 1)\n",
    "\n",
    "    # Annotate first, middle and last point with the corresponding threshold value\n",
    "    for i in [0, (len(fpr_list) // 2) - 1, len(fpr_list) - 1]:  \n",
    "        plt.text(fpr_list[i], tpr_list[i], str(round(VERIFICATION_THRESHOLDS[i], 1)), \n",
    "                fontsize=9, ha='right', va='bottom', color='red')\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the ROC curve to find suitable thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [test_case[2] for test_case in all_test_cases] # Create list consisting of the correct classifiers\n",
    "\n",
    "plot_ROC(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8 # The best threshold value according to the ROC curve above\n",
    "\n",
    "plot_conf_mat(y_true, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
