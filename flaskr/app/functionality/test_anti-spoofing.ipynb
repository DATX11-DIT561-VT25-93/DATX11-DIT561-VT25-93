{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b7d481",
   "metadata": {},
   "source": [
    "### 1. Functions and variables for collecting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179db89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from detection import detect_face_for_testing\n",
    "from feature_extraction import predict_spoof\n",
    "from anti_spoof import load_antispoof_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH_REAL = \"LCC_eval_pics/real\"\n",
    "DATASET_PATH_ATTACK = \"LCC_eval_pics/spoof\"\n",
    "SAVE_PATH = 'results_FAS/'\n",
    "N_REAL = 314\n",
    "N_SPOOFED = 700\n",
    "SPOOF_THRESHOLDS = np.concatenate(([0], np.arange(0.94, 1.00, 0.01)))\n",
    "\n",
    "antispoof_sess, antispoof_input = load_antispoof_model()\n",
    "\n",
    "def get_image_paths(dataset_path, n_images):\n",
    "    # Get n random image paths from the dataset\n",
    "    image_paths = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png'))]\n",
    "    sampled_image_paths = np.random.choice(image_paths, n_images, replace=False)\n",
    "\n",
    "    return sampled_image_paths\n",
    "\n",
    "def get_predictions(dataset_path, correct_pred, thresholds, n_pred):\n",
    "    n_undetected = 0\n",
    "    y_pred_li = [[] for _ in range(len(thresholds))] # Initialize a list of predictions for each threshold\n",
    "    \n",
    "    image_paths = get_image_paths(dataset_path, n_pred)\n",
    "\n",
    "    for image_path in image_paths: \n",
    "        image_array = cv2.imread(image_path)\n",
    "\n",
    "        if image_array is None:\n",
    "            print(f\"Error reading image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        face_data, image_rgb = detect_face_for_testing(image_array)\n",
    "\n",
    "        if face_data is not None:\n",
    "\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                # Check if the image is spoofed based on the face data\n",
    "                pred = predict_spoof(face_data, image_rgb, antispoof_sess, antispoof_input, threshold)\n",
    "\n",
    "                if pred != correct_pred:\n",
    "                    #print(f'Incorrect prediction at: {image_path}')\n",
    "                    pass\n",
    "\n",
    "                y_pred_li[i].append(pred) # Append the prediction to the list for the current threshold\n",
    "        else:\n",
    "            n_undetected += 1\n",
    "            #print(f\"No face detected in image: {image_path}\")\n",
    "        \n",
    "    print(f\"Number of undetected images: {n_undetected}\")\n",
    "    return y_pred_li, n_undetected\n",
    "\n",
    "def run_test(path_real, path_attack, thresholds, n_real, n_spoofed, test_n):\n",
    "    # Collect one list of predictions for non-spoofed images for every threshold\n",
    "    y_pred_real_li, n_undetected_real = get_predictions(path_real, True, thresholds, n_real)\n",
    "    y_true_real = np.full(len(y_pred_real_li[0]), True)\n",
    "    print(f\"Number of real images: {len(y_pred_real_li[0])}\")\n",
    "\n",
    "    # Collect one list of predictions for spoofed images for every threshold\n",
    "    y_pred_attack_li, n_undetected_attack = get_predictions(path_attack, False, thresholds, n_spoofed)\n",
    "    y_true_attack = np.full(len(y_pred_attack_li[0]), False)\n",
    "    print(f\"Number of spoofed images: {len(y_pred_attack_li[0])}\")\n",
    "\n",
    "    y_pred_li = [y_pred_real + y_pred_attack for (y_pred_real, y_pred_attack) in zip(y_pred_real_li, y_pred_attack_li)]\n",
    "    y_true = np.concatenate((y_true_real, y_true_attack))\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for y_pred, threshold in zip(y_pred_li, thresholds):\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        rows.append({\n",
    "            'Test Number': test_n,\n",
    "            'Threshold': threshold,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Undetected Real Images': n_undetected_real,\n",
    "            'Undetected Spoofed Images': n_undetected_attack\n",
    "        })\n",
    "\n",
    "        print(f\"Threshold: {threshold:.2f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\\n\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return y_pred_li, y_true, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ad9e6",
   "metadata": {},
   "source": [
    "### 2. Collecting predictions on images for each threshold, then plotting ROC curves and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_conf_mat, plot_ROC, get_tpr_and_fpr\n",
    "\n",
    "test_n_li = np.arange(1, 11) # Number of tests to run\n",
    "\n",
    "total_test_data = pd.DataFrame()\n",
    "\n",
    "tpr_li_li = []\n",
    "fpr_li_li = []\n",
    "\n",
    "for test_n in test_n_li:\n",
    "    print(f\"Test number: {test_n}\")\n",
    "\n",
    "    # Get a list of predictions and performance data for each threshold\n",
    "    y_pred_li, y_true, test_data = run_test(DATASET_PATH_REAL, DATASET_PATH_ATTACK, SPOOF_THRESHOLDS, N_REAL, N_SPOOFED, test_n)\n",
    "    total_test_data = pd.concat([total_test_data, test_data], ignore_index=True)\n",
    "\n",
    "    # Visualize the results using ROC curve\n",
    "    plot_ROC(y_true, y_pred_li, SPOOF_THRESHOLDS, test_n)\n",
    "\n",
    "    # Get TPR and FPR for each test to later compute average ROC curve\n",
    "    tpr_li = []\n",
    "    fpr_li = []\n",
    "\n",
    "    for y_pred in y_pred_li:\n",
    "        tpr, fpr = get_tpr_and_fpr(y_true, y_pred)\n",
    "        tpr_li.append(tpr)\n",
    "        fpr_li.append(fpr)\n",
    "\n",
    "    tpr_li_li.append(tpr_li)\n",
    "    fpr_li_li.append(fpr_li)\n",
    "    \n",
    "    # Visualize the results for each threshold using confusion matrices\n",
    "    for i, y_pred in enumerate(y_pred_li):\n",
    "        print(f\"Confusion matrix for threshold {SPOOF_THRESHOLDS[i]:.2f}:\")\n",
    "        plot_conf_mat(y_true, y_pred, SPOOF_THRESHOLDS[i], test_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6de483",
   "metadata": {},
   "source": [
    "#### Plotting the average ROC curve across all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2208ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_rates(rate_li_li):\n",
    "    rates_df = pd.DataFrame(rate_li_li)\n",
    "    average_rates = list(rates_df.mean(axis=0))\n",
    "\n",
    "    return average_rates\n",
    "\n",
    "avg_tpr_li = get_average_rates(tpr_li_li)\n",
    "avg_fpr_li = get_average_rates(fpr_li_li)\n",
    "\n",
    "# Visualize the average ROC curve\n",
    "plot_ROC(y_true, y_pred_li, SPOOF_THRESHOLDS, 'AVG', avg_rates=(avg_tpr_li, avg_fpr_li))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46952c5c",
   "metadata": {},
   "source": [
    "### 3. Analyze and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45131531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average performance metrics for each threshold across all tests\n",
    "total_test_data = total_test_data.drop(columns=[\"Test Number\"])\n",
    "average_metrics = total_test_data.groupby(\"Threshold\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Inspect the average metrics\n",
    "print(\"Average metrics across all tests:\")\n",
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to CSV and Excel files\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "total_test_data.to_csv(f\"{SAVE_PATH}test_results_FAS.csv\", index=False)\n",
    "total_test_data.to_excel(f\"{SAVE_PATH}test_results_FAS.xlsx\", index=False)\n",
    "\n",
    "average_metrics.to_csv(f\"{SAVE_PATH}average_metrics_FAS.csv\", index=False)\n",
    "average_metrics.to_excel(f\"{SAVE_PATH}average_metrics_FAS.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
